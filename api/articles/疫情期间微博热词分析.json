{"title":"疫情期间微博热词分析","slug":"疫情期间微博热词分析","date":"2020-12-17T05:39:06.827Z","updated":"2020-12-17T13:16:20.983Z","comments":true,"path":"api/articles/疫情期间微博热词分析.json","photos":[],"link":"","excerpt":null,"covers":null,"content":"<h1 id=\"scrapy框架简介\"><a href=\"#scrapy框架简介\" class=\"headerlink\" title=\"scrapy框架简介\"></a>scrapy框架简介</h1><ul>\n<li><a href=\"https://scrapy.org/\">scrapy</a>是一个开源的数据挖掘框架。</li>\n<li>这个框架主要实现的功能是从网页上下载所需要的信息。</li>\n<li>其原理，简单来讲，是通过模拟登陆，来模拟人眼观看信息的行为，然后收集所需信息。</li>\n<li>框架共有五个组件：引擎（scrapy engine，控制整个爬虫工作的进程）、爬虫（spider，用于从特定网页获取所需信息）、实体管道（item pipeline，处理爬虫获取的实体（entity），主要处理的方法是持久化实体、验证实体有效性、清除不需要的信息）、调度器（scheduler、是爬取网址的队列，决定爬的顺序/优先级）、下载器（downloader，下载网络数据）。</li>\n<li>这五个部分的运行流程是（与人的上网行为相对应）：</li>\n</ul>\n<ol>\n<li>spider-request-URL-engine-scheduler（人类输入网址，请求访问）</li>\n<li>schedule排序-engine-Downloader（多个网址，排列访问的优先级，然后请求下载相关数据）</li>\n<li>Downloader-request-internet-engine（从互联网下载的信息交给人脑）</li>\n<li>response-spider-engine（人类处理信息）</li>\n<li>engine-pipeline（人脑决定将这些数据放在某处（pipeline）保存）</li>\n</ol>\n<ul>\n<li>scrapy中间件介绍：<br>  scrapy有两个中间件，一个连接的是engine和downloader，一个连接的是engine和spider，前者负责加工信息（给访问请求加上cookie等，便于更逼真地模拟访问），后者负责过滤信息（滤掉无效信息）。</li>\n</ul>\n<h1 id=\"如何用scrapy挖掘微博热词\"><a href=\"#如何用scrapy挖掘微博热词\" class=\"headerlink\" title=\"如何用scrapy挖掘微博热词\"></a>如何用scrapy挖掘微博热词</h1>","categories":[],"tags":[]}