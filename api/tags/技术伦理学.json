{"name":"技术伦理学","slug":"技术伦理学","count":1,"postlist":[{"title":"关于深度自然语言处理（Deep NLP）中偏见情况的调查","slug":"deep nlp","date":"2021-04-13T01:13:58.376Z","updated":"2021-04-13T07:54:03.189Z","comments":true,"path":"api/articles/deep nlp.json","excerpt":"","keywords":null,"cover":"D:\\frozenfish123.github.io\\source\\pictures","content":"<p>本文是预印本，于2021年3月2日发布在<a href=\"www.preprint.org\"></a>，未经同行评议。文章标题为A Survey on Bias in Deep NLP，作者单位是西班牙哈恩大学信息与技术高等研究院。</p>\n<p>摘要：深度神经网络（Deep neural networks）是目前机器学习领域，包括自然语言处理领域（NLP）在内，最为主流的方法。各种通用性强、性能良好的深度模型为神经网络提供了新的设计，它们之所以能够持续发布，是由于大型语料库的可用性以及深度架构在自监督学习过程（或者说“预训练”）中塑造内部语言机制的能力。通过训练集，这些网络模型会学习一些词汇和关系的概率分布，但这个学习过程保持黑箱状态，不可解释。在这样的学习过程中，这些模型会集成一些所使用的训练集中的潜在缺陷、断裂和偏见。鉴于这样的预训练模型（pre-trained models）被发现是迁移学习中非常有效的方法，处理这些学习产生的偏见也就成为了一个新的问题。我们用一种正规的方式来引入“偏见”的概念，并探索在不同的网络之中，偏见是如何被处理的，如何被监测和更正的；同时，我们还确定了可利用的资源，并提出了处理深度自然语言处理中偏见问题的对策。</p>\n<p>关键词：自然语言处理；深度学习；偏见模型</p>\n<h1 id=\"0-引言\"><a href=\"#0-引言\" class=\"headerlink\" title=\"0.引言\"></a>0.引言</h1><p>在社会学中，偏见（bias）是一种对于某个人或某个族群有利或不利的预判（prejudice），这种预判往往被认为是不公平的。从一方面来说，它是一个及其普遍的现象，但另一方面来说，也正是因为这种现象的普遍性，深度神经网络在有意识地发现数据中模式的过程中，由于这些数据本身具有一些偏差，他们在处理计算机视觉、音频处理、文本处理等任务的过程中，也会学习到偏见。所有这些领域都属于自动决策系统（automated decision systems）的一部分。自动决策系统，指的是一些目标是自动化、辅助或者替代人类决策的软件、系统或者一些程序。自动决策系统可以包括一些工具，这些工具可以被用来分析数据集，以此为依据来打分、预测、分类或者推荐。这些自动决策系统被应用的场合包括但不限于教育机会、健康状况、工作表现、职业机会、社会流动、兴趣爱好、行为或者个人自主性评估，涉及到我们生活的一些感性的层面。</p>\n<p>在这种情况下，有偏见的人工智能模型会作出一些偏向于某些特定群体的决策。Obermeyer等人发现，美国医院用于分配病人的医疗服务系统一直包含对于黑人系统性的歧视：比起同样生病的白人，这个系统很少介绍黑人，以让同样具有复杂医药需求的后者的治疗方案得以改进。在计算机视觉领域，一些人脸识别算法不能识别出黑人用户，或者将黑人用户标注为“大猩猩”。在语音识别领域，人们发现，语音听写系统识别男性的声音比识别女性的声音更准确。此外，在预测刑事累犯时，风险评估系统可能会预判某些种族的人更有可能犯罪。</p>\n<p>在深度自然语言处理(deep NLP)领域，词嵌入及相关的语言模型现在被大量使用。这些模型通常来自互联网，在大型数据库上进行训练。在这个过程中，可能会对刻板的偏见知识进行编码，并产生偏见的语言。这种带有语言偏见的情况可能出现在对话助手和聊天机器人中。在简历审查系统中，由于存在偏见，系统将女性应聘者列为计算机编程工作资格较低的人。Caliskan等人提出了词嵌入（Word Embedding）联想测试(WEAT)作为一种检查概念之间的词嵌入的关联的方法。在社会心理学领域的隐性联想测试（IAT）中捕捉到的，旨在评估测试对象所持有的隐性定型观念，如不自觉地联想到黑人的定型名字，或者与黑人刻板印象一致的词语。</p>\n<p>这个问题还远远没有得到缓解，更不要说被解决。目前，没有什么规范的文档制定流程来告知语言模型在学习行为中的特点，尽管在提供透明的模型报告方面已经有了一些努力，如模型卡或者数据声明的提供。此外，新的模型使用在训练过程中使用的数据集变得越来越大，他们能够很好地抓住文档中的潜在语义，而且这些偏见成为了新模型的一部分。现存的最先进的GPT-3语境语言模型就是这样的情况。它使用了1750亿个参数，比使用15亿个参数的GPT-2多100多倍。基于此，Brown等人报告了其在社会偏差方面的发现，更简明扼要地阐述了性别、种族和宗教问题。<strong>性别偏见</strong>的研究方法是观察性别和职业之间的各种联系：他们发现，根据GPT-3的计算，在接受测试的388种职业中，83%的职业更有可能是与男性身份识别器（male identifier）有关；此外，需要更高的受教育程度的职业，例如银行家、名誉教授等等，极大程度上向男性倾斜，而诸如助产士、护士、接待员和管家等都是向女性倾斜。<strong>种族偏见</strong>是通过观察种族如何影响情绪来进行探讨：结果表明，亚裔种族对情绪的影响一直很高，而非裔种族的情绪一直很低。最后，通过观察哪些词与下列宗教相关的宗教术语一起出现探讨了<strong>宗教偏见</strong>：例如，”暴力”、”恐怖主义”、”恐怖分子 “等词与伊斯兰教的关联度比其他宗教的比例高。这一发现与[16]的工作报告一致。当GPT-3被要求用一个包含 “穆斯林”一词的短语完成一个句子的时候，在百分之六十的情况下，它的造句将穆斯林与枪击、炸弹、谋杀或暴力联系起来。</p>\n<p>本文对NLP中的偏见进行了正式的定义，并详尽介绍了最近年来解决这一问题的相关著作。确定了偏见研究的主要课题并进行了讨论。本文其余部分的结构如下：首先介绍了一个关于偏见（bias）的正式定义，以及它在机器学习，尤其是语言模型中的含义；然后，我们对偏见检测、评估和校正的现状进行回顾；第四节是我们的建议用于处理深度NLP中的偏差，更具体地说，是处理语言模型中的偏差的一般方法论的产生和应用；最后，我们得出一些结论，并确定主要的研究挑战。</p>\n<h1 id=\"1-定义偏见\"><a href=\"#1-定义偏见\" class=\"headerlink\" title=\"1.定义偏见\"></a>1.定义偏见</h1><p>认知科学对不同类型偏见的研究已经进行了四十多年。从一开始，人们就发现偏见是人类与生俱来的决策策略。当认知偏见（cognitive bias）被应用时，我们就会假定现实按照某种认知可能根本不是真的，但我们可以据此形成判断。一个偏见可以通过不完全归纳法获得，或是来自他人的（教育或观察）。在任何情况下，一种偏见都会提供一种远离逻辑推理的思维方式。目前已发现的认知偏见有百余种，这些偏见分属于社会、行为、记忆相关等多个领域。其中有一个是我们要重点关注的：刻板印象。</p>\n<p>如果认知偏见可以被定义为人类认知通过某种明确的思维路径连贯地产生了一些在某些方面系统性扭曲了客观事实的表征，那么刻板印象可以定义为是指根据民族、种族或族裔或性别群体而对他人施加的某些特征的假设。因此，刻板印象因为该个体属于某个群体而赋予个人某些特征。某种程度上，它就像一种本体论，在其中，仅仅因为个人拥有某种在某些属性上的特殊价值（如她带有在“性别”属性中带有“女性”价值，或者他在“种族”属性上带有“非裔”价值）而被使用某种特定分类规则（所以某些特定的属性被假定出来，例如傲慢、弱点或者犯罪行为）。可以看出，刻板印象能够在语义层面上使用一个形式化的方案来建模，正如那些在知识工程领域被提出的本体语言。</p>\n<p>我们先介绍一下公平性，因为它是机器学习中一个非常有名的概念（相当于 “零偏见”系统），以及此方面的一些处理措施。然后我们将讨论公平性措施如何帮助我们处理语言模型中的偏见问题。在本节结束时，我们提出了一个正式定义。</p>\n<h2 id=\"1-1-机器学习中的偏见问题\"><a href=\"#1-1-机器学习中的偏见问题\" class=\"headerlink\" title=\"1.1 机器学习中的偏见问题\"></a>1.1 机器学习中的偏见问题</h2><p>与偏见密切相关的一个概念是公平。如果一个制度的结果不因性别或国籍等某些属性而具有歧视性，则该制度被认为是“公平的”。在机器学习评估中，可以根据不同保护组的混淆矩阵来估计区分度。也就是说，我们可以计算混淆矩阵和导出率（阳性率、真阳性率、假阳性率等），作为对某一特征（如“性别”）上的完整样本集合的分割。如果这些比率远远不相等，那就有可能证明预测系统存在“不公平”行为，即，对如何根据该特征的值作出决策具有明显的偏见。已经提出了一些措施来研究不同人口群体预测率之间的差异，如何根据每个系统目标来解释这些差异现在已经明确。从认知偏差中衍生出的大量偏差来看，大约有几十种偏差对机器学习问题产生了兴趣。后两项研究汇集了在分析机器学习系统中的偏见问题时已达成一致的若干措施，这些措施包括人口均等、机会均等、机会均等或反事实公平等。当然，这些措施可以应用于许多人工智能子领域，如图像识别或自然语言处理，让我们看看其中一个的定义（人口均等），因为在语言建模中，一些元素可以转移到我们对偏见的正式定义中。</p>\n<p>人口均等规定，受保护阶层的不同价值观（如性别）所产生的所有群体都应获得相同的积极成果率（编者按：积极成果是指一些如升学、求得职位等结果）。例如，如果系统决定以相同的比例向男女群体中的人提供奖学金，那么系统将显示人口均等。假设ˆY是关于奖学金应该授予（ˆY=1）还是拒绝（ˆY=0）的预测决定。那么，人口均等可以定义为P（Y=1 | A=0）=P（Y=1 | A=1），这相当于男性和女性的阳性率相等PR（A=0）=PR（A=1）。这里，ˆY是系统预测，A是“protected”属性/类。在我们的示例中，这是性别，其可能值为0（对于男性）或1（对于女性）。当然，这一措施可以推广到任何受保护的阶级，如种族或国籍。在这种情况下，如果所有可能的人口段的阳性率相等，那么公平就被赋予了。偏见在哪里？它就在那里，因为偏差是由于受保护属性的不同值而导致的组之间的偏差。因此，偏倚将是偏倚=| P（ˆY=1 | A=0）-P（ˆY=1 | A=1）|，这等于偏倚=| PR（A=0）-PR（A=1）|使用人口平价作为估计量。机会均等是公平的一个很好的估量。这一条考虑了真阳性率之间的相等性。正如所指出的，其余的衡量标准都是我们希望从不同的分数中得到的相等的变量。</p>\n<p>一般来说，公平性是在分布&lt;X，A，Z，Y，ˆY&gt;上计算的，X表示样本，A表示受保护属性，Z表示其余属性，Y表示这些样本的真实标签，ˆY表示模型预测的标签。公平性的明确定义及其评估方式允许在学习过程中引入纠正机制，如FairTorch Library 1中实施的机制。正如我们所看到的，这种接近偏差校正的方法接近于所谓的统计偏差。我们在学习过程中引入偏差最小化作为附加约束。</p>\n<p>公平（fairness）不是一种认知偏见，这与统计建模中的参数估计有关，而神经网络就是这样做的。但公平，某种程度上说，是减少机器学习中刻板印象的措施的形式化。根据维基百科2，统计偏差是统计技术或其结果的一个特征，即结果的预期值不同于所估计的真正的基本定量参数。事实上，公平性度量是统计偏差的度量。因此，每当一个受保护的特征被清楚地识别出来或者可以从训练集中的样本特征中得到时，就有可能对模型的公平性进行评估，以便在由受保护特征的不同值产生的组上生成类似的预测分布。即使在自然语言处理中，许多任务可以用机器学习来定义，但当受保护的属性在数据集中不是一个明确的特征时，仍然是一个挑战。训练前如何定义偏见/公平？我们如何在像GPT-2或BERT这样通过语言建模方法训练的模型上测量公平性？我们将在下一节中对此问题提出一个答案。</p>\n<h2 id=\"1-2-一个关于语言模型偏见的反思\"><a href=\"#1-2-一个关于语言模型偏见的反思\" class=\"headerlink\" title=\"1.2 一个关于语言模型偏见的反思\"></a>1.2 一个关于语言模型偏见的反思</h2><p>语言模型（LM）估计单词序列P（w1，…，wm）的概率。对于给定的单词序列，这个模型能够估计下一个最可能的单词。模型参数学习背后的机制可用于解决许多不同的任务，如机器翻译、文本生成、文本分类或标记（如命名实体识别）等。 偏见存在于语言模型中，正如它存在于人类中一样。偏见是人类语言固有的，它并不总是不公平的根源。一辆满是故障的车容易出事故；科幻电影的粉丝愿意看类似的电影；患有慢性病的患者病情恶化的风险更大，等等。我们所谓的“不公平”是建立在一个很高的语义层次上的。记住，<strong>偏见不是关于预测误差，而是关于语义期望的偏差行为</strong>。</p>\n<p><em>定义1 语言模型中的刻板印象偏差是指，该语言模型中某些词的概率分布随给定领域中某些优先词（prior words）的变化，这些变化是不符合期待的。</em></p>\n<p>这些优先词是可以链接到受保护属性的术语。在“性别”范畴内，这些词可以是女演员、女人、女孩等。也就是说，在语言模型中，我们期望单词woman后面的概率分布在某些单词上与单词man相等（或非常接近），比如那些与专业技能相关的单词。男人和女人都是性别领域的特定词汇（受保护的属性）。它提出了一个问题，即准确地定义域和那些期望的“特定”词。在这个例子之后，性别领域中的词语将被分成两个不同的类别，在这两个类别中，人们倾向于出现刻板印象，一个类别是男性（演员、服务员……），另一个类别是女性（女演员、服务员……）。然后，在这种情况下，关于专业技能属性（智力、效率、清洁度、创造力……）的词语可以用来分析它们如何在与每个类相关联的不同概率分布上出现，也就是说，当域中的词语作为分布的先验出现时。所以，我们可以确定“创造力”这个词在男人面前的概率和女人面前的概率是不同的。</p>\n<p>这种基于语言建模的偏见现象分析方法表明，偏见本身并不是语言模型的缺陷，它只是生成模型的数据和模型在语义层面的期待行为的影响。因此，由语言工程师决定哪些领域和哪些预期的分布必须被监视或最终被纠正。为此，必须在领域内确定刻板印象，并且必须选择那些刻板印象所偏向的相关属性或概念。为了克服偏见问题的清晰定义，我们提出了一种基于本体的方法，首先在语义层对偏差问题进行识别，然后在模型参数层对偏差问题进行处理。</p>\n<h2 id=\"1-3-语义层面对偏见的定义\"><a href=\"#1-3-语义层面对偏见的定义\" class=\"headerlink\" title=\"1.3 语义层面对偏见的定义\"></a>1.3 语义层面对偏见的定义</h2><p>描述逻辑为知识库的结构、填充和操作提供了一套完整的元素。它实际上是语义网及其高级本体术语OWL所获得的本体形式化。OWL本体有以下组件：&lt;C，P，I，L&gt;类C，属性P，个体I和文本值L。为了简单起见，我们总结说个体是类的实例，实例通过属性相互关联，文本通过属性与个体相关联。例如，克里斯蒂娜是一个“类型”女性（属于阶级女性）的个体。她在医院工作。她有一份医生的工作。Woman是一个类，可以通过表达式has gender“female”（这在OWL中称为类表达式）来定义，其中has gender是一个属性，“female”是一个文本值。这个简单的知识可以用图形表示，如图1所示。</p>\n<p><img src=\"D:\\frozenfish123.github.io\\source\\pictures\"></p>\n<p>现在是时候借用机器学习中公平性度量的一些术语和OWL的一些元素了。</p>\n<p><em>定义2 定型知识由元组&lt;C，P，I，L，pp，Ps&gt;表示，其中C是类集，P是属性集，I是个体集，L是文字集，pp∈p是保护属性，Ps∈P是定型属性集。这表明由不同价值观的保护财产pp形成的个体群体在刻板印象财产Ps的价值分布上可能表现出不平等。</em></p>\n<p>在图1所示的示例中，我们可以将has gender视为受保护属性pp，Ps=fhas jobg视为构造型属性集，因此元组将是&lt;C，P，I，L，has gender，fhas jobg&gt;。根据定义2，这意味着财产和工作的价值不能平均分配给由性别定义的两个阶级的个人。例如，我们可能会发现，对于性别为“女性”的个体，观察有工作的托儿所比观察有工作的医生更频繁，而持有性别为“男性”的个体的班级则相反。值得注意的是，刻板的知识只是定义了一种潜在的偏见，即我们可以感知的偏见。</p>\n<h2 id=\"1-4-语言模型对于偏见的定义\"><a href=\"#1-4-语言模型对于偏见的定义\" class=\"headerlink\" title=\"1.4 语言模型对于偏见的定义\"></a>1.4 语言模型对于偏见的定义</h2><p>一旦明确了刻板印象偏见的语义定义，我们就可以将语义识别映射到词的概率。这很简单，因为语言模型只不过是一个能够计算单词序列P（w1，…，wm）概率的模型。</p>\n<p><em>定义3 定型语言可以表示为元组&lt;C、P、I、L、pp、Ps、Tp、Ts&gt;，其中包含定型知识和两个术语集：受保护的术语Tp和定型术语Ts。受保护的术语Tp是词汇表中可以明确映射到受保护属性pp值的表达式（单词或多个词）。定型术语Ts是词汇表中的那些表达式（单词或多个单词），可以明确地映射到原型属性Ps的值。</em></p>\n<p>Ts是一组词或术语，它们代表了刻板印象属性Ps的可能值（例如，“高想象力”、“低感性”、“美”、“理性思维”等）。Tp中的表达可以是定义性别的任何术语，例如“护士”、“女演员”、“女人”、“女孩”或类似的词语。一旦在语义层面上定义了刻板印象，我们就可以认为，如果原型属性Ps上Ts中包含表达式的一系列单词的概率与被引用个体的pp值有显著差异，那么该模型是有偏的。现在，我们已经为语言模式中的定型偏见的最终定义做好了准备。</p>\n<p><em>定义4 设Ls=&lt;C，P，I，L，pp，Ps，Tp，Ts&gt;为带有刻板印象的语言的定义，刻板印象偏见定义为概率d（P（w1，…，wm|tiP），P（w1，…，wm|tjP））之间的距离d，其中i≠j，tiP和tjP是受保护属性pp，存在wk∈{w1，…，wm}的两个不同值的表达式，因此wk∈Ts。</em></p>\n<p>换句话说，如果包含刻板表达的术语的概率分布与现有的受保护表达先验不同，则语言模型是有偏差的。按照我们的简单例子，刻板印象语言可以定义为&lt;C，P，I，L，has gender，fhas jobg，fgirl，women，Christine，mang，fdoctor，nurseg&gt;。</p>\n<p>现在，考虑一下这个简单的文本：Christine works as a nurse in the hospital. A man is the doctor.这个定义对任何距离都是开放的。如果我们选择绝对差异，则在上述文本上训练的语言模型的刻板印象偏差可能是：|P（works，as，a，nurse|Christine）-P（works，as，a，doctor|Christine）|，另一个有效的衡量标准是：|P（works，as，a，nurse|man）-P（works，as，a，doctor）|。如您所见，可以根据所考虑的受保护属性的顺序或优先级值来计算不同的距离。因此，对语言模型的适当评估意味着一系列类似于上述表达式的表达式，以受保护的表达式作为优先项，并在序列中使用定刻板印象表达式，由此可以计算出平均距离。</p>\n<h1 id=\"2-偏见相关研究的综述\"><a href=\"#2-偏见相关研究的综述\" class=\"headerlink\" title=\"2. 偏见相关研究的综述\"></a>2. 偏见相关研究的综述</h1><p>这部分暂时先跳过，作者用了文献计量学的方法，非常充分地完成了这个综述。可以参考原文。</p>\n<h1 id=\"3-讨论\"><a href=\"#3-讨论\" class=\"headerlink\" title=\"3. 讨论\"></a>3. 讨论</h1><p>尽管上面的表格根据确定的维度详细说明了偏倚研究的关键方面，但现在对所有这些丰富的研究成果进行了更深入的分析。我们将讨论分为以下几个突出的主题。</p>\n<h2 id=\"3-1-联想测试（association-test）\"><a href=\"#3-1-联想测试（association-test）\" class=\"headerlink\" title=\"3.1 联想测试（association test）\"></a>3.1 联想测试（association test）</h2><p>有几种测量和缓解偏差的方法。Bolukbasi等人为以后的许多工作奠定了基础。主要的贡献是在表明嵌入捕捉到了术语之间的相关性，因此可以正确地解决类比，如男人：国王→女人：女王，但也有一些类似的类比是有偏差的。例如，它关联男人：医生和女人：护士，而关联女人：医生会更充分。使用同样的机制，得到了一组对每种性别都有定型的术语，以证明这不是一个孤立的案例。为了消除这种偏见，他们建议找到性别向量子空间方向，并调整向量，使职业术语不分性别。</p>\n<h1 id=\"4-处理深度自然语言处理模型中的偏见问题的通用方法论\"><a href=\"#4-处理深度自然语言处理模型中的偏见问题的通用方法论\" class=\"headerlink\" title=\"4. 处理深度自然语言处理模型中的偏见问题的通用方法论\"></a>4. 处理深度自然语言处理模型中的偏见问题的通用方法论</h1><h1 id=\"5-结论和挑战\"><a href=\"#5-结论和挑战\" class=\"headerlink\" title=\"5. 结论和挑战\"></a>5. 结论和挑战</h1>","text":"本文是预印本，于2021年3月2日发布在，未经同行评议。文章标题为A Survey on Bias in Deep NLP，作者单位是西班牙哈恩大学信息与技术高等研究院。摘要：深度神经网络（Deep neural networks）是目前机器学习领域，包括自然语言处理领域（NLP","link":"","raw":null,"photos":[],"categories":[],"tags":[{"name":"技术伦理学","slug":"技术伦理学","count":1,"path":"api/tags/技术伦理学.json"}]}]}